{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76aeddfd-044f-4f1c-b03c-92e2ac5ef883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Processing - Bronze Layer\n",
    "\n",
    "When creating long-term storage for analytical use cases, the first step is to **ingest data** from the **source**, with a shape as close as possible to the original shape. As the first step in our data processing journey.\n",
    "\n",
    "We will:\n",
    "* Ingest the raw data in a single pull\n",
    "* Convert the data to parquet format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eca25c2-99d2-4d0e-9bb8-1187ccc7a8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up this Notebook\n",
    "Before we get started, we need to quickly set up this notebook by installing a helpers, cleaning up your unique working directory (as to not clash with others working in the same space), and setting some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca09d57a-2a7c-4cc1-8ca5-0c0897dcccdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y databricks_helpers exercise_ev_databricks_unit_tests\n",
    "%pip install git+https://github.com/data-derp/databricks_helpers#egg=databricks_helpers git+https://github.com/data-derp/exercise_ev_databricks_unit_tests#egg=exercise_ev_databricks_unit_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3851863-2be4-4abc-bcf1-5a133a1638ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_helpers.databricks_helpers import DataDerpDatabricksHelpers\n",
    "exercise_name = \"batch_processing_bronze_ingest\"\n",
    "helpers = DataDerpDatabricksHelpers(dbutils, exercise_name)\n",
    "\n",
    "current_user = helpers.current_user()\n",
    "working_directory = helpers.working_directory()\n",
    "print(f\"Your current working directory is: {working_directory}\")\n",
    "\n",
    "## This function CLEARS your current working directory. Only run this if you want a fresh start or if it is the first time you're doing this exercise.\n",
    "helpers.clean_working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f65898b-17a7-400a-93c8-fb3b0dbc9449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read OCPP Data\n",
    "We've done this a couple of times before! Run the following cells to download the data to local storage and create a DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1423443-1a20-40fa-8e63-92e7a99ebae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/kelseymok/charge-point-simulator-v1.6/main/out/1680355141.csv.gz\"\n",
    "filepath = helpers.download_to_local_dir(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ce8939-f126-4290-9c35-e201d79a164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def create_dataframe(filepath: str) -> DataFrame:\n",
    "    \n",
    "    custom_schema = StructType([\n",
    "        StructField(\"message_id\", StringType(), True),\n",
    "        StructField(\"message_type\", IntegerType(), True),\n",
    "        StructField(\"charge_point_id\", StringType(), True),\n",
    "        StructField(\"action\", StringType(), True),\n",
    "        StructField(\"write_timestamp\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .option(\"escape\", \"\\\\\") \\\n",
    "        .schema(custom_schema) \\\n",
    "        .load(filepath)\n",
    "    return df\n",
    "    \n",
    "df = create_dataframe(filepath)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2f9359-f84f-4d69-9e2a-4b2ad6dd01f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EXERCISE: Write to Parquet\n",
    "Now that we have our ingested data represented in a DataFrame, let's use the [`parquet writer`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html) along with [`mode=\"overwrite\"`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html) to formally write our data to the specified `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52fe608-9b55-4a50-aa74-726a77a27f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write(input_df: DataFrame):\n",
    "    out_dir = f\"{working_directory}/output/\"\n",
    "    \n",
    "### Put your code here.\n",
    "    mode_name = \"overwrite\"\n",
    "    \n",
    "    input_df. \\\n",
    "        write. \\\n",
    "        mode(mode_name). \\\n",
    "        parquet(out_dir)\n",
    "    \n",
    "    \n",
    "write(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b983d8b9-d795-40ce-9c93-0dd7a523185f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's inspect what we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f547bbf-d14d-431c-8ffc-6d73d49a4fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(f\"{working_directory}/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31504209-a64d-4192-8b6e-6fcfd9e3325e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A bit of clean up before we move on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b25e26-98fc-42fd-b6e2-5902589f58f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "helpers.clean_working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a005e98d-cfac-4b28-bbe7-2c0ac814cfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Thoughtworks. All rights reserved.<br/>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
